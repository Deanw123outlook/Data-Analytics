{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a7e41e",
   "metadata": {},
   "source": [
    "## Microsoft Azure Logging\n",
    "\n",
    "NOTE\n",
    "\n",
    "This was commercial work completed to help build out tooling for proactive infastrucutre analysis and problem detection from Microsoft Azure data. The dataset used has been generated in Microsfot Excel to mirror close to the original quantitative data. Furthermore, categorical naming and datetime data has been changed.\n",
    "\n",
    "SYSTEM INFASTRUCTURE BACKGROUND\n",
    "\n",
    "The specific type of logging indicates the servers hit by client traffic/requests for subject specific information to be returned across Microsoft Azure Cloud Infastructure. Specific clients will be aligned with Azure clusters/engine-nodes. Cluster will hold include certain engine nodes at different periods in time dependent on customer/client traffic expectations.\n",
    "\n",
    "Server\n",
    "We will not desribe the particular servers functionality in this case. However, what is import to understand is that the serves which are assigned to specific nodes in this instance were being hit to return in house functionality. This Python script will not detail this functionality. What is important is the usecase of the code to pull out specific information from our data.\n",
    "\n",
    "Using our logging tooling we can acquire upto 15 days logs for specific system infastructure logging aimed at both proactive and reactive problem detection and solution.\n",
    "\n",
    "Data\n",
    "\n",
    "- Date : log date\n",
    "- EngineNode : Specific engine node used across & assinged to specific clusters for specific clients\n",
    "- Client : Live client|customer\n",
    "- RequestSize(TB) : Custumer|Client Server Request Size - Terabytes\n",
    "- Milliseconds : Time milliseconds to return speific server function to client/customer\n",
    "- Seconds : Time seconds to return speific server function to client/customer\n",
    "- Minutes : Time minutes to return speific server function to client/customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de1991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # import numpy package for numerical python\n",
    "import pandas as pd # import pandas\n",
    "\n",
    "# import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px \n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import normalize, StandardScaler # data pre-processing libraries and modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e23f9",
   "metadata": {},
   "source": [
    "#### DATA CLEANING & FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataframe from CSV file & parse dates\n",
    "data = pd.read_excel('DataDecodedPrepared.xlsx', parse_dates = True)\n",
    "\n",
    "#create copy of orginal dataframe\n",
    "data_copy = data.copy(deep = True)\n",
    "\n",
    "# drop column\n",
    "data.drop(data.columns[[6]], axis=1, inplace=True)\n",
    "\n",
    "# strng strip (remove part of string)\n",
    "data['Milliseconds'] = data['Milliseconds'].str.rstrip('ms')\n",
    "# remove white space\n",
    "data['Client'] = data['Client'].str.strip()\n",
    "data['EngineNode'] = data['EngineNode'].str.strip()\n",
    "\n",
    "#covert 'Date' column object to datetime object\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "#\n",
    "data['Milliseconds'] = data['Milliseconds'].astype('int64')\n",
    "\n",
    "#new column milliseconds conversion to seconds\n",
    "data['Seconds'] = data['Milliseconds']/1000\n",
    "\n",
    "#new column seconds conversion to minutes\n",
    "data['Minutes'] = data['Milliseconds']/60000\n",
    "\n",
    "# copy dataframe - pre-processing (later in analysis)\n",
    "pre_process = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8012cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bf900",
   "metadata": {},
   "source": [
    "#### LOG FREQUENCY\n",
    "\n",
    "We will need to count each alert (len(df)) as an individual alert which is the case, however in this dataset we do not have this specific column/series data. We can do this with further feature engineering be creating a new column and using the index to increment the value of the index on each iteration by 1 using the dataframe.\n",
    "\n",
    "NOTE\n",
    "- Its important to remember zero indexing for this task.\n",
    "\n",
    "VISUALIZATION\n",
    "- Simply, we can then plot this quantitative discrete data against time using our datetime column in the pandas dataframe. We will be using a histogram to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AlertFrequency'] = data.index + 1 #  dataframe index, can be integer numbers or string values, the column labels, called column names, are usually strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly express line chart\n",
    "fig = px.histogram(data_frame = data, x = 'Date', y = 'AlertFrequency',color_discrete_sequence=['blue'],\n",
    "                   opacity=0.6, width=1000, height=500 ,histnorm=\"probability\")\n",
    "fig.update_layout(\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    "    paper_bgcolor=\"LightSteelBlue\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdaa7f1",
   "metadata": {},
   "source": [
    "In terms of subject matter this is highly important information as we can see when our pricing software was under most constraint within this time period. This can then be cross analysed against further findings below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0cbef",
   "metadata": {},
   "source": [
    "#### DESCRIPTIVE STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c32639",
   "metadata": {},
   "source": [
    "## Correlation Analysis -  Client Request Size (TB) Terabytes Server Return Time Taken Functionality  Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e6134",
   "metadata": {},
   "source": [
    "We can test the current correlation of the mentioned variables over the datasets timeframe which is approximitely 15 days. This alone will not be completely useful. However, if we can gauge here what the correaltion is and test over differing time periods this could provide useful for key infasturcutre system health and problem detections. \n",
    "\n",
    "Statistical Correlation Analysis\n",
    "\n",
    "Correlation analysis in research is a statistical method used to measure the strength of the linear relationship between two variables and compute their association. Simply put - correlation analysis calculates the level of change in one variable due to the change in the other.\n",
    "\n",
    "The correlation coefficient is the unit of measurement used to calculate the intensity in the linear relationship between the variables involved in a correlation analysis, this is easily identifiable since it is represented with the symbol r and is usually a value without units which is located between 1 and -1.\n",
    "\n",
    "Correlation between two variables can be either a positive correlation, a negative correlation, or no correlation. Let's look at examples of each of these three types.\n",
    "\n",
    "- Positive correlation: A positive correlation between two variables means both the variables move in the same direction. An increase in one variable leads to an increase in the other variable and vice versa.\n",
    "- Negative correlation: A negative correlation between two variables means that the variables move in opposite directions. An increase in one variable leads to a decrease in the other variable and vice versa.\n",
    "- Weak/Zero correlation: No correlation exists when one variable does not affect the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8549a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scatterplot (discrete quantitative data accross time)\n",
    "fig = px.scatter(data_frame=data,x = 'Date',y = 'RequestSize(TB)',color_discrete_sequence=['green'],\n",
    "                   opacity=0.6,width=1000, height=650)\n",
    "fig.update_layout(\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    "    paper_bgcolor=\"lightsteelblue\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbba8c8",
   "metadata": {},
   "source": [
    "We already know our variable of interest displayed on the y axis is discrete quantitative data. This means the data includes nondivisible figures which you can statistically count. The above plot would be helpful with OUTLIER DETECTION however in this instance this is not of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b26a00",
   "metadata": {},
   "source": [
    "The below lineplot will allow is to visually analyse the line trend across time and both the variables of interest. Here, we can get a visually rough idea of Pearsons Correlation between the two mentioned variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lineplot using plotly\n",
    "fig = make_subplots(rows=2, cols=1,subplot_titles=(\"Time Series Request Size (Terabytes) Analysis\", \"Time Series Server Request Return Time  in Seconds\"))\n",
    "#\n",
    "fig.add_trace(go.Scatter(x=data['Date'], y=data['RequestSize(TB)']),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=data['Date'], y=data['Seconds']),\n",
    "              row=2, col=1)\n",
    "\n",
    "# Update xaxis properties\n",
    "fig.update_xaxes(title_text=\"Date|Time\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Date|Time\", row=2, col=1)\n",
    "# Update yaxis properties\n",
    "fig.update_yaxes(title_text=\"RequestSize(TB)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Seconds\", row=2, col=1)\n",
    "\n",
    "# Update title and height\n",
    "fig.update_layout(height=900, width=1500,\n",
    "                  title_text=\"Statistical Correlation Analysis\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# pearsons correlation\n",
    "print(\"RequestSize(TB) v Seconds Correlation Analysis: \", data['RequestSize(TB)'].corr(data['Seconds'])) # statsitical correlation method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17741e15",
   "metadata": {},
   "source": [
    "We can see that there is a negative correlation of -0.55, meaning that the distibution of the two variables move in opposite directions. An increase in one variable leads to a decrease in the other variable and vice versa. We can conclude that the linear relationship between the two variables is off medium strength based on our correaltion calculated value.\n",
    "\n",
    "HOWEVER, as both variables have both differing units of measurements one being time and the other being count data, the question arises how can we compare both these will differ in terms of numerical value. \n",
    "The above correlation analysis may well be untrue/miss-leading. \n",
    "\n",
    "### Data Pre-Processing\n",
    "\n",
    "In this instance we want to test pre-processing our data to regulate the magnitude of both variables values to see if there is any change in the correlation value generated. Simply put this method will aim to treat the values of both variables as the same unit of measurement. \n",
    "- Normalising the data has been chosen for sample testing. What would be required is in depth knowledge of pre-processing techniques and use cases. This would require further testing and analysis. \n",
    "\n",
    "Normalization has been chosen as this type of pre-processing has been worked with before. We could tst other pre-processing techniques however we will not be doing this for this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64db392",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process.drop(['Date','Host','Service','EngineNode','Client','Milliseconds','Minutes','AlertFrequency'],axis = 1, inplace=True) # drop dataframe columns\n",
    "pre_process.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Standard Scaler Function\n",
    "scaler = StandardScaler() \n",
    "data_scaled = scaler.fit_transform(pre_process) \n",
    "  \n",
    "# Normalizing the data so that the data approximately follows a Gaussian distribution \n",
    "data_normalized = normalize(data_scaled) \n",
    "  \n",
    "# Converting the numpy array into a pandas DataFrame \n",
    "data_normalized = pd.DataFrame(data_normalized) \n",
    "  \n",
    "# Renaming the columns \n",
    "data_normalized.columns = pre_process.columns \n",
    "  \n",
    "data_normalized.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=1,subplot_titles=(\"Time Series Request Size (Terabytes) Analysis\", \"Time Series Server Request Return Time  in Seconds\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=data_normalized.index, y=data_normalized['RequestSize(TB)']),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=data_normalized.index, y=data_normalized['Seconds']),\n",
    "              row=2, col=1)\n",
    "\n",
    "# Update xaxis properties\n",
    "fig.update_xaxes(title_text=\"Date|Time\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Date|Time\", row=2, col=1)\n",
    "\n",
    "# Update yaxis properties\n",
    "fig.update_yaxes(title_text=\"RequestSize(TB) - Terabytes\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Seconds\", row=2, col=1)\n",
    "\n",
    "# Update title and height\n",
    "fig.update_layout(height=900, width=1500,\n",
    "                  title_text=\"Statistical Correlation Analysis\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#\n",
    "print(\"RequestSize(TB) v Seconds Correlation Analysis: \", data_normalized['RequestSize(TB)'].corr(data_normalized['Seconds'])) # statsitical correlation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06081765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearsons correlation\n",
    "print(\"RequestSize(TB) v Seconds Correlation Analysis: \", data['RequestSize(TB)'].corr(data['Seconds'])) # statsitical correlation method\n",
    "print(\"RequestSize(TB) v Seconds Correlation Analysis: \", data_normalized['RequestSize(TB)'].corr(data_normalized['Seconds'])) # statsitical correlation method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7de3ff",
   "metadata": {},
   "source": [
    "#### Correlation Analysis Conclusion\n",
    "- We can see that from our testing that the correlation value returned against both these features indicates a medium strenght correlction. This potentially indicates that request-sze in terabytes can be correlated to an increase in request time in seconds being returned. Aligning this with subject knowledge this can be seen as justifiable as certain (x) will generate more traffic based on certain dates. However, what would be require is further testing over a longer period and cross comparitive analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
